# -*- coding: utf-8 -*-
"""Project Solution - Plant Seedlings Image Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ggUljd_ezZA5nIMk4glARm2repdH8t7_

# Project: Plant Seedlings Classicication.

### Data Description:

- You are provided with a dataset of images of plant seedlings at various stages of grown.
- Each image has a filename that is its unique id.
- The dataset comprises 12 plant species.
- The goal of the project is to create a classifier capable of determining a plant's species from a photo.

### Dataset:

- The dataset can be download from Olympus.
- The data file names are:
    - images.npy
    - Label.csv
- The original files are from Kaggle. Due to the large volume of data, the images were converted to images.npy file and the labels are also put into the Labels.csv. So that you can work on the data/project seamlessly without worrying about the high data volume.
- Link to the Kaggle project site: https://www.kaggle.com/c/plant-seedlings-classification/data?select=train

**Note: For project purposes, download the data provided on Olympus**


### Context:

- Can you differentiate a weed from a crop seedling?
- The ability to do so effectively can mean better crop yields and better stewardship of the environment.
- The Aarhus University Signal Processing group, in collaboration with University of Southern Denmark, has recently released a dataset containing images of unique plants belonging to 12 species at several growth stages.

### Objective:

- To implement the techniques learnt as a part of the course.

### Learning Outcomes:
- Pre-processing of image data.
- Visualization of images.
- Building CNN.
- Evaluate the Model.
"""



# Import necessary libraries.
import cv2
import math
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

from tensorflow.keras import models, layers
from tensorflow.keras.optimizers import RMSprop, Adam
from keras.utils.np_utils import to_categorical # convert to one-hot-encoding

# # Load the image file of dataset
# images = np.load('/content/drive/My Drive/Colab Notebooks/data/plant_seedlings_numpy/images.npy')

# # Load the labels file of dataset
# labels = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/plant_seedlings_numpy/Labels.csv')

from google.colab import drive
drive.mount('/content/drive')

import os
os.getcwd()

os.chdir('/content/drive/MyDrive/Great Learning/data')

!wget --header='Host: lms-uploads.s3-ap-southeast-1.amazonaws.com' --header='User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 11_0_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36' --header='Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9' --header='Accept-Language: en-US,en;q=0.9' --header='Referer: https://olympus.greatlearning.in/' 'https://lms-uploads.s3-ap-southeast-1.amazonaws.com/account_1/attachments/995385/images.npy?response-content-disposition=attachment%3B%20filename%3D%22images.npy%22%3B%20filename%2A%3DUTF-8%27%27images.npy&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIGH64THF2Z2BWILA%2F20210109%2Fap-southeast-1%2Fs3%2Faws4_request&X-Amz-Date=20210109T160033Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=67071d59002c2984a92fdb735750e8458d554ede31ff8e49cc263297adb23bc2' -c -O 'images.npy'

!wget --header='Host: lms-uploads.s3-ap-southeast-1.amazonaws.com' --header='User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 11_0_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36' --header='Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9' --header='Accept-Language: en-US,en;q=0.9' --header='Referer: https://olympus.greatlearning.in/' 'https://lms-uploads.s3-ap-southeast-1.amazonaws.com/account_1/attachments/996273/Labels.csv?response-content-disposition=attachment%3B%20filename%3D%22Labels.csv%22%3B%20filename%2A%3DUTF-8%27%27Labels.csv&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIGH64THF2Z2BWILA%2F20210109%2Fap-southeast-1%2Fs3%2Faws4_request&X-Amz-Date=20210109T160329Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=5710dfe30ab53ea33ae9dece586f2ab086e39bca965a1f0900770b0131eb8c9d' -c -O 'Labels.csv'

images = np.load('images.npy')
labels = pd.read_csv('Labels.csv')

print(images.shape)
print(labels.shape)

labels

# Show some example images
for index,i in enumerate(range(8)):
    
    plt.imshow(images[i])
    plt.title(labels['Label'][index])
    plt.show()

round(labels['Label'].value_counts(normalize=True)*100,2)

# Show some example images
for i in range(8):
    plt.subplot(2, 4, i + 1)
    plt.imshow(images[i])

"""# Apply image processing on the images:

- Gaussian Blurring.
- Normalization.
- Visualize data after pre-processing.
"""

preprocessed_images = []       # Initialize empty list to store the images after pre-processing.

for img in images:
    # Use gaussian blur
    blurImg = cv2.GaussianBlur(img, (5, 5), 0)   
     
    
    preprocessed_images.append(blurImg)        # Append image 
    
    # Show examples

plt.subplot(1, 3, 1); plt.imshow(img)         # Show the original image
plt.subplot(1, 3, 2); plt.imshow(blurImg)     # Blur image

preprocessed_images = np.asarray(preprocessed_images)    # Convert to numpy array.

plt.subplot(1, 3, 1); plt.imshow(images[1])         # Show the original image
plt.subplot(1, 3, 2); plt.imshow(preprocessed_images[1])     # Blur image

# preprocessed_train = []       # Initialize empty list to store the images after pre-processing.

# for img in images:
#     # Use gaussian blur
#     blurImg = cv2.GaussianBlur(img, (5, 5), 0)   
     
    
#     preprocessed_train.append(blurImg)        # Append image 
    
#     # Show examples

# plt.subplot(1, 3, 1); plt.imshow(img)         # Show the original image
# plt.subplot(1, 3, 2); plt.imshow(blurImg)     # Blur image

# preprocessed_train = np.asarray(preprocessed_train)    # Convert to numpy array.

lower=(25,40,50)
upper=(75,255,255)

img_v2=images[1]
blurred=cv2.GaussianBlur(img_v2,(5,5),0)
hsv=cv2.cvtColor(blurred,cv2.COLOR_BGR2HSV)

mask=cv2.inRange(hsv,lower,upper)
struc=cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(11,11))
mask=cv2.morphologyEx(mask,cv2.MORPH_CLOSE,struc)


boolean=mask>0
new=np.zeros_like(img_v2,np.uint8)
new[boolean]=img_v2[boolean]

plt.subplot(2, 3, 1); plt.imshow(img_v2)         # Show the original image
plt.subplot(2, 3, 2); plt.imshow(blurred)
plt.subplot(2, 3, 3); plt.imshow(hsv)
plt.subplot(2, 3, 4); plt.imshow(mask)
plt.subplot(2, 3, 5); plt.imshow(boolean)
plt.subplot(2, 3, 6); plt.imshow(new)


complex

# Show sample result
for i in range(8):
  lower=(25,40,50)
  upper=(75,255,255)

  img_v2=images[i]
  blurred=cv2.GaussianBlur(img_v2,(5,5),0)
  hsv=cv2.cvtColor(blurred,cv2.COLOR_BGR2HSV)

  mask=cv2.inRange(hsv,lower,upper)
  struc=cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(11,11))
  mask=cv2.morphologyEx(mask,cv2.MORPH_CLOSE,struc)


  boolean=mask>0
  new=np.zeros_like(img_v2,np.uint8)
  new[boolean]=img_v2[boolean]

  plt.subplot(1, 3, 1); plt.imshow(img_v2)         # Show the original image
  plt.subplot(1, 3, 2); plt.imshow(new)
  plt.show()
  #plt.subplot(2, 3, 3); plt.imshow(hsv)
  #plt.subplot(2, 3, 4); plt.imshow(mask)
  #plt.subplot(2, 3, 5); plt.imshow(boolean)
  #plt.subplot(2, 3, 6); plt.imshow(new)
    
    #plt.subplot(2, 4, i + 1)
    #plt.imshow(preprocessed_train[i])

preprocessed_train = []       # Initialize empty list to store the images after pre-processing.

for img in images:
    # Use gaussian blur
    blurImg = cv2.GaussianBlur(img, (5, 5), 0)   
     
    
    preprocessed_train.append(blurImg)        # Append image 
    
    # Show examples

plt.subplot(1, 3, 1); plt.imshow(img)         # Show the original image
plt.subplot(1, 3, 2); plt.imshow(blurImg)     # Blur image

preprocessed_train = np.asarray(preprocessed_train)    # Convert to numpy array.

# Show sample result
for i in range(8):
    plt.subplot(2, 4, i + 1)
    plt.imshow(preprocessed_images[i])

# Normalize image data.
preprocessed_images = preprocessed_images / 255

"""# Make data compatible:

- Convert labels from digits to one hot vectors.
- Print the label for y_train[0].
- Check the shape of data, Reshape data into shapes compatible with Keras models, if already not compatible.

"""

# Convert labels from digits to one hot vectors.

from sklearn.preprocessing import LabelBinarizer
enc = LabelBinarizer()
y = enc.fit_transform(labels)

y[0]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(preprocessed_images, y, test_size=0.3, random_state=42)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

# Reshape data into shapes compatible with Keras models.

X_train = X_train.reshape(X_train.shape[0], 128, 128, 3)
X_test = X_test.reshape(X_test.shape[0], 128, 128, 3)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

"""So, we can see above that the data was already compatible with Keras, as the shape of data before and after reshaping is same."""

random_seed = 2
from sklearn.model_selection import train_test_split
X_test, X_val, y_test, Y_val = train_test_split(X_test, y_test, test_size = 0.5, random_state=random_seed)

print(X_test.shape)
print(X_val.shape)

"""# Building CNN:

- Define layers.
- Set optimizer and loss function. (Use Adam optimizer and categorical crossentropy)
"""

# Set the CNN model

batch_size = None

model = models.Sequential()
model.add(layers.Conv2D(32, (5, 5), padding='same', activation="relu", input_shape=(128, 128, 3)))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Dropout(0.2))
model.add(layers.Conv2D(64, (5, 5), padding='same', activation="relu"))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Dropout(0.3))
model.add(layers.Conv2D(64, (3, 3), padding='same', activation="relu"))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Dropout(0.4))
model.add(layers.Conv2D(64, (3, 3), padding='same', activation="relu"))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Dropout(0.5))

model.add(layers.GlobalMaxPooling2D())
model.add(layers.Dense(128, activation="relu"))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(12, activation="softmax"))

model.summary()

# Set the optimizer and loss function, and compile the model with them.

optimizer = Adam(
                  learning_rate=0.001,
                  beta_1=0.9,
                  beta_2=0.999,
                  epsilon=1e-07,
                  amsgrad=False,
                  name='Adam')
model.compile(optimizer = optimizer, loss="categorical_crossentropy", metrics=["accuracy"])

# Set epochs = 1000, and fit the model
#early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5)

history = model.fit(X_train, y_train, epochs = 40, validation_data = (X_val,Y_val),batch_size = batch_size)

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right');

# Evaluate the model.

score = model.evaluate(X_test, y_test, verbose=0, batch_size = 38)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

# Predict the values from the validation dataset
Y_pred = model.predict(X_val)
# Convert predictions classes to one hot vectors 
result = np.argmax(Y_pred, axis=1)
# Convert validation observations to one hot vectors
Y_true = np.argmax(Y_val, axis=1)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

conf_mat = confusion_matrix(Y_true, result)

df_cm = pd.DataFrame(conf_mat, index = [i for i in range(0, 12)],
                  columns = [i for i in range(0, 12)])
plt.figure(figsize = (10,7))
sns.heatmap(df_cm, annot=True, fmt='g');

import numpy as np

plt.figure(figsize=(2,2))
plt.imshow(X_test[3],cmap="gray")
plt.show()
print('Predicted Label', np.argmax(model.predict(X_test[3].reshape(1,128,128,3))))
print('True Label', np.argmax(y_test[3]))

plt.figure(figsize=(2,2))
plt.imshow(X_test[2],cmap="gray")
plt.show()
print('Predicted Label', np.argmax(model.predict(X_test[2].reshape(1,128,128,3))))
print('True Label', np.argmax(y_test[2]))

plt.figure(figsize=(2,2))
plt.imshow(X_test[33],cmap="gray")
plt.show()
print('Predicted Label', np.argmax(model.predict(X_test[33].reshape(1,128,128,3))))
print('True Label', np.argmax(y_test[33]))

plt.figure(figsize=(2,2))
plt.imshow(X_test[59],cmap="gray")
plt.show()
print('Predicted Label', np.argmax(model.predict(X_test[59].reshape(1,128,128,3))))
print('True Label', np.argmax(y_test[59]))

plt.figure(figsize=(2,2))
plt.imshow(X_test[36],cmap="gray")
plt.show()
print('Predicted Label', np.argmax(model.predict(X_test[36].reshape(1,128,128,3))))
print('True Label', np.argmax(y_test[36]))

